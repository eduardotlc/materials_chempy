{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8a0659-92a0-4878-a59e-b67a20abf345",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Machine Learning algorithm\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## Importing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ec62d12-ba8b-48ff-9017-e1a5cb918728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee676d-b4a9-4d6b-929a-c2fd233884b2",
   "metadata": {},
   "source": [
    "## Defining functions\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0a4a26-51d0-4a92-bd6d-9b77f78306ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuations at the beginning or end\n",
    "    word = word.strip(string.punctuation)\n",
    "    word = word.lower()  # Convert to lowercase\n",
    "    if word.endswith(\"s\"):\n",
    "        word = word[:-1]  # Remove the trailing 's'\n",
    "    return word\n",
    "\n",
    "\n",
    "def most_common_word_in_list(input_text, word_list):\n",
    "    words = input_text.lower().split()  # Convert to lowercase before splitting\n",
    "    filtered_words = [\n",
    "        preprocess_word(word) for word in words if preprocess_word(word) in word_list\n",
    "    ]\n",
    "    word_count = Counter(filtered_words)\n",
    "    if word_count:\n",
    "        return word_count.most_common(1)[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320e494-3da1-4145-9090-3e03f5e09905",
   "metadata": {},
   "source": [
    "## Extracting and spliting pdf strings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3645c4-609f-4bf3-936b-490d4ac03ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "pdf_words = extract_text_from_pdf(\"./test.pdf\")\n",
    "pdf_words = pdf_words.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a950ed2-ed84-4e7a-b35a-0fce667ca83d",
   "metadata": {},
   "source": [
    "## Creating categories and training data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c021f84d-e8b3-470a-8014-1302519be6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a  # Define your categories and their corresponding data paths\n",
    "categories = [\"category1\", \"category2\", \"category3\"]\n",
    "data_paths = {\n",
    "    \"SO\": [\n",
    "        \"/home/eduardotc/Zotero/storage/T8A7CK3F/Kuk et al. - 2023 - Singlet-oxygen-driven photocatalytic degradation o.pdf\",\n",
    "        \"/home/eduardotc/Zotero/storage/MABMTDMW/Linger et al. - 2023 - Evaluation of relative efficiency of PDT photosens.pdf\",\n",
    "        \"/home/eduardotc/Zotero/storage/KJ3YHB9R/Krajczewski et al. - 2019 - Role of various nanoparticles in photodynamic ther.pdf\",\n",
    "    ],\n",
    "    \"X-ray\": [\n",
    "        \"/home/eduardotc/Zotero/storage/VGS9VKPU/Shaheen et al. - 2018 - In Vitro Cytotoxicity and Morphological Assessment.pdf\",\n",
    "        \"/home/eduardotc/Zotero/storage/H7ZYSJCH/Clement et al. - 2016 - X-ray induced singlet oxygen generation by nanopar.pdf\",\n",
    "        \"/home/eduardotc/Zotero/storage/P37C94Y4/Bulin et al. - 2013 - X-ray-Induced Singlet Oxygen Activation with Nanos.pdf\",\n",
    "    ],\n",
    "    \"h2o2\": [\n",
    "        \"/home/eduardotc/Zotero/storage/LE68YYAB/Yang et al. - 2023 - Packing-induced selectivity switching in molecular.pdf\",\n",
    "        \"/home/eduardotc/Zotero/storage/6JUNK2BN/Xu et al. - 2023 - Boosting alkaline photocatalytic H2O2 generation b.pdf\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create a list of (text, category) tuples for training\n",
    "training_data = []\n",
    "for category, files in data_paths.items():\n",
    "    for file in files:\n",
    "        text = extract_text_from_pdf(file)\n",
    "        training_data.append((text, category))\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text_clf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "texts = [data[0] for data in training_data]\n",
    "categories = [data[1] for data in training_data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, categories, test_size=0.2)\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebee00-0503-4b24-b709-93f1443692c6",
   "metadata": {},
   "source": [
    "## Classifying a PDF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287bd472-abb1-46d9-b994-e692d4f868de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pdf(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    predicted_category = text_clf.predict([text])[0]\n",
    "    return predicted_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a952609-2bc5-4ed6-974f-9a3a305d7f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO\n"
     ]
    }
   ],
   "source": [
    "tst2 = classify_pdf(\n",
    "    \"/home/eduardotc/Zotero/storage/C84JU3XU/Chayanun et al. - 2020 - Direct Three-Dimensional Imaging of an X-ray Nanof.pdf\"\n",
    ")\n",
    "print(tst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6b8bb-c759-40e4-b92b-a26f72b3a55d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Most common words classifications\n",
    "---\n",
    "---\n",
    "\n",
    "## Imports and definitions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b58663-505c-4a95-848e-33c62eb4f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "\n",
    "def is_numeric(word):\n",
    "    try:\n",
    "        float(word)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuations at the beginning or end\n",
    "    word = word.strip(string.punctuation)\n",
    "    word = word.lower()  # Convert to lowercase\n",
    "    if word.endswith(\"s\"):\n",
    "        word = word[:-1]  # Remove the trailing 's'\n",
    "    return word\n",
    "\n",
    "\n",
    "def merge_rows(group):\n",
    "    merged_value = sum(group[\"numbers\"].astype(float))\n",
    "    return pd.DataFrame({\"word\": [group[\"word\"].iloc[0]], \"numbers\": [merged_value]})\n",
    "\n",
    "\n",
    "def most_common_words(input_string, n, excluded_words):\n",
    "    words = input_string.split()\n",
    "    filtered_words = [\n",
    "        word for word in words if word not in excluded_words and not is_numeric(word)\n",
    "    ]\n",
    "    word_count = Counter(filtered_words)\n",
    "    return word_count.most_common(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0ad04-08ca-40ea-89b1-8769a4959670",
   "metadata": {},
   "source": [
    "## User defined variables\n",
    "---\n",
    "\n",
    "- pdf_path = String, pdf file path.\n",
    "\n",
    "- higher_ns = Integer, Number of total most common words to be separated.\n",
    "\n",
    "- excluded_words = dict, pdf words to be excluded from analysis\n",
    "\n",
    "    - List of non importatn strings, to be filtered from the file results, mainly conjunctions, determiners, punctuation, personal names and initials, etc ...\n",
    "\n",
    "    - *Numbers are filtered from the main strings list by the most_common_words function, not being necessary to add them to the bellow function.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f28d91a4-825d-4ac7-a8f5-4fcb1c4276cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "          Categories Subcategories\n",
      "0            singlet        breast\n",
      "1               cell          lung\n",
      "2             cancer         liver\n",
      "3       nanoparticle        kidney\n",
      "4                MOF            Au\n",
      "5             oxygen            Ag\n",
      "6       luminescence            Nb\n",
      "7   mechanochemistry            Pd\n",
      "8          catalysis  fluorescense\n",
      "9                NaN  upconversion\n",
      "10               NaN           nir\n",
      "11               NaN         laser\n",
      "12               NaN         light\n",
      "13               NaN          mass\n",
      "14               NaN          HPLC\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pdf_path = \"/home/eduardotc/Zotero/storage/ILZX7FWI/Farhana et al. - 2023 - Gold Nanoparticles Inhibit PMA-Induced MMP-9 Expre.pdf\"\n",
    "sort_categories = [\n",
    "    \"singlet\",\n",
    "    \"cell\",\n",
    "    \"cancer\",\n",
    "    \"nanoparticle\",\n",
    "    \"MOF\",\n",
    "    \"oxygen\",\n",
    "    \"luminescence\",\n",
    "    \"mechanochemistry\",\n",
    "    \"catalysis\",\n",
    "]\n",
    "sort_subcategories = [\n",
    "    \"breast\",\n",
    "    \"lung\",\n",
    "    \"liver\",\n",
    "    \"kidney\",\n",
    "    \"Au\",\n",
    "    \"Ag\",\n",
    "    \"Nb\",\n",
    "    \"Pd\",\n",
    "    \"fluorescense\",\n",
    "    \"upconversion\",\n",
    "    \"nir\",\n",
    "    \"laser\",\n",
    "    \"light\",\n",
    "    \"mass\",\n",
    "    \"HPLC\",\n",
    "]\n",
    "\n",
    "excluded_words = {\n",
    "    \"the\",\n",
    "    \"of\",\n",
    "    \"in\",\n",
    "    \"and\",\n",
    "    \"to\",\n",
    "    \".\",\n",
    "    \"a\",\n",
    "    \"is\",\n",
    "    \"by\",\n",
    "    \"this\",\n",
    "    \"for\",\n",
    "    \"with\",\n",
    "    \"No\",\n",
    "    \"be\",\n",
    "    \"as\",\n",
    "    \"The\",\n",
    "    \"can\",\n",
    "    \"at\",\n",
    "    \"that\",\n",
    "    \"are\",\n",
    "    \"it\",\n",
    "    \"A.\",\n",
    "    \"B.\",\n",
    "    \"C.\",\n",
    "    \"D.\",\n",
    "    \"E.\",\n",
    "    \"F.\",\n",
    "    \"G.\",\n",
    "    \"H.\",\n",
    "    \"I.\",\n",
    "    \"J.\",\n",
    "    \"K.\",\n",
    "    \"L.\",\n",
    "    \"M.\",\n",
    "    \"N.\",\n",
    "    \"O.\",\n",
    "    \"P.\",\n",
    "    \"Q.\",\n",
    "    \"R.\",\n",
    "    \"S.\",\n",
    "    \"T.\",\n",
    "    \"U.\",\n",
    "    \"V.\",\n",
    "    \"X.\",\n",
    "    \"Y.\",\n",
    "    \"Z.\",\n",
    "    \"on\",\n",
    "    \"or\",\n",
    "    \"e\",\n",
    "    \"O\",\n",
    "    \"et\",\n",
    "    \"This\",\n",
    "    \"has\",\n",
    "    \"In\",\n",
    "    \"di\",\n",
    "    \"very\",\n",
    "    \"al.\",\n",
    "    \"been\",\n",
    "    \"Yes\",\n",
    "    \"have\",\n",
    "    \"which\",\n",
    "    \"from\",\n",
    "    \"will\",\n",
    "    \"an\",\n",
    "    \",\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \"one\",\n",
    "    \"other\",\n",
    "    \"A\",\n",
    "    \"but\",\n",
    "    \"Section\",\n",
    "    \"more\",\n",
    "    \"most\",\n",
    "    \"these\",\n",
    "    \"It\",\n",
    "    \"was\",\n",
    "    \"not\",\n",
    "    \"its\",\n",
    "    \"some\",\n",
    "    \"al.,\",\n",
    "    \"As\",\n",
    "    \"κ\",\n",
    "    \"Figure\",\n",
    "    \"These\",\n",
    "    \"signiﬁcantly\",\n",
    "    \"used\",\n",
    "    \"were\",\n",
    "    \"compared\",\n",
    "    \"also\",\n",
    "    \"(Figure\",\n",
    "    \"CrossRef\",\n",
    "}\n",
    "\n",
    "sort_df = {\"Categories\": sort_categories}\n",
    "sort_df = pd.DataFrame(sort_df, index=idxcat)\n",
    "print(\"\\n\")\n",
    "sortsub_df = {\"Subcategories\": sort_subcategories}\n",
    "sortsub_df = pd.DataFrame(sortsub_df, index=idxsubcat)\n",
    "merged_df = pd.merge(sort_df, sortsub_df, how='outer', left_index=True, right_index=True)\n",
    "print(merged_df)\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26cbaa-4f9c-41d9-bba5-d2e22a795ada",
   "metadata": {},
   "source": [
    "## Strings separation and counting\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8e89399-1de4-4197-bc86-eaae42092ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fd9e1\">\n",
       "  <caption>Subcategories</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fd9e1_level0_col0\" class=\"col_heading level0 col0\" >words</th>\n",
       "      <th id=\"T_fd9e1_level0_col1\" class=\"col_heading level0 col1\" >numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fd9e1_row0_col0\" class=\"data row0 col0\" >breast</td>\n",
       "      <td id=\"T_fd9e1_row0_col1\" class=\"data row0 col1\" >70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fd9e1_row1_col0\" class=\"data row1 col0\" >lung</td>\n",
       "      <td id=\"T_fd9e1_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fd9e1_row2_col0\" class=\"data row2 col0\" >liver</td>\n",
       "      <td id=\"T_fd9e1_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fd9e1_row3_col0\" class=\"data row3 col0\" >kidney</td>\n",
       "      <td id=\"T_fd9e1_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fd9e1_row4_col0\" class=\"data row4 col0\" >Au</td>\n",
       "      <td id=\"T_fd9e1_row4_col1\" class=\"data row4 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_fd9e1_row5_col0\" class=\"data row5 col0\" >Ag</td>\n",
       "      <td id=\"T_fd9e1_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_fd9e1_row6_col0\" class=\"data row6 col0\" >Nb</td>\n",
       "      <td id=\"T_fd9e1_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_fd9e1_row7_col0\" class=\"data row7 col0\" >Pd</td>\n",
       "      <td id=\"T_fd9e1_row7_col1\" class=\"data row7 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_fd9e1_row8_col0\" class=\"data row8 col0\" >fluorescense</td>\n",
       "      <td id=\"T_fd9e1_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_fd9e1_row9_col0\" class=\"data row9 col0\" >upconversion</td>\n",
       "      <td id=\"T_fd9e1_row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_fd9e1_row10_col0\" class=\"data row10 col0\" >nir</td>\n",
       "      <td id=\"T_fd9e1_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_fd9e1_row11_col0\" class=\"data row11 col0\" >laser</td>\n",
       "      <td id=\"T_fd9e1_row11_col1\" class=\"data row11 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_fd9e1_row12_col0\" class=\"data row12 col0\" >light</td>\n",
       "      <td id=\"T_fd9e1_row12_col1\" class=\"data row12 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_fd9e1_row13_col0\" class=\"data row13 col0\" >mass</td>\n",
       "      <td id=\"T_fd9e1_row13_col1\" class=\"data row13 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e1_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_fd9e1_row14_col0\" class=\"data row14 col0\" >HPLC</td>\n",
       "      <td id=\"T_fd9e1_row14_col1\" class=\"data row14 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9d45943810>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_string = extract_text_from_pdf(pdf_path)\n",
    "word_list = []\n",
    "count_list = []\n",
    "catls = []\n",
    "numsum = []\n",
    "common_words = most_common_words(input_string, n, excluded_words)\n",
    "for word, count in common_words:\n",
    "    word_list.append(preprocess_word(word))\n",
    "    count_list.append(float(count))\n",
    "df = {\"word\": word_list, \"numbers\": count_list}\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "grouped = df.groupby(\"word\")\n",
    "result_df = grouped.apply(merge_rows).reset_index(drop=True)\n",
    "categories_count = []\n",
    "ts = 0\n",
    "\n",
    "for b in merged_df[\"Categories\"]:\n",
    "    ts = df.loc[df[\"word\"] == b]\n",
    "    ts = pd.DataFrame(ts)\n",
    "    tts = sum(ts[\"numbers\"])\n",
    "    numsum.append(tts)\n",
    "    catls.append(b)\n",
    "dffin = {\"words\": catls, \"numbers\": numsum}\n",
    "dffin = pd.DataFrame(dffin)\n",
    "dffin = dffin.style.set_caption(\"Categories\")\n",
    "\n",
    "catls = []\n",
    "numsum = []\n",
    "\n",
    "for c in merged_df[\"Subcategories\"]:\n",
    "    ts = df.loc[df[\"word\"] == c]\n",
    "    ts = pd.DataFrame(ts)\n",
    "    tts = sum(ts[\"numbers\"])\n",
    "    numsum.append(tts)\n",
    "    catls.append(c)\n",
    "    \n",
    "dffin2 = {\"words\": catls, \"numbers\": numsum}\n",
    "dffin2 = pd.DataFrame(dffin2)\n",
    "dffin2 = dffin2.style.set_caption(\"Subcategories\")\n",
    "dffin\n",
    "dffin2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfclassify",
   "language": "python",
   "name": "pdfclassify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
